# -*- coding: utf-8 -*-
"""ashwam_exercise_b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SCYznjbdgwPiHuVKMU80qO9SUTStNkvl
"""

from google.colab import drive
drive.mount('/content/drive')

BASE_DIR = "/content/drive/MyDrive/ashwam_exercise_b"

import os

folders = [
    "data/llm_runs",
    "src"
]

for f in folders:
    os.makedirs(os.path.join(BASE_DIR, f), exist_ok=True)

BASE_DIR

import json
import os
from collections import defaultdict

DATA_DIR = os.path.join(BASE_DIR, "data")

def load_journals():
    journals = []
    path = os.path.join(DATA_DIR, "journals.jsonl")
    with open(path, "r") as f:
        for line in f:
            journals.append(json.loads(line))
    return journals

def load_llm_runs():
    runs_dir = os.path.join(DATA_DIR, "llm_runs")
    runs = defaultdict(dict)

    for fname in os.listdir(runs_dir):
        if not fname.endswith(".json"):
            continue

        # Example: journal_1_run_2.json
        parts = fname.replace(".json", "").split("_")
        journal_id = parts[1]
        run_id = parts[3]

        with open(os.path.join(runs_dir, fname), "r") as f:
            runs[journal_id][run_id] = json.load(f)

    return runs

!ls /content/drive/MyDrive

!ls /content/drive/MyDrive/ashwam_exercise_b

!ls /content/drive/MyDrive/ashwam_exercise_b/data

path = os.path.join(DATA_DIR, "journals.json")

print("Looking for journals at:", path)
print("Files in data/:", os.listdir(DATA_DIR))

def load_journals():
    path = os.path.join(DATA_DIR, "journals.jsonl")
    if not os.path.exists(path):
        print("journals.jsonl not found — proceeding without journal text")
        return []

    journals = []
    with open(path, "r") as f:
        for line in f:
            journals.append(json.loads(line))
    return journals

!ls /content/drive/MyDrive/ashwam_exercise_b/data/llm_runs

# Stability configuration & assumptions

STABLE_FIELDS = {
    "type",        # emotion / symptom / etc.
    "polarity",    # present / absent (HIGH RISK)
    "evidence"     # grounding span
}

DRIFT_ALLOWED_FIELDS = {
    "intensity",
    "arousal",
    "time_bucket"
}

# Thresholds (conservative by design)
EVIDENCE_OVERLAP_THRESHOLD = 0.3
SEMANTIC_SIM_THRESHOLD = 0.85

def span_overlap(span_a, span_b):
    """
    span = {"start": int, "end": int}
    """
    start = max(span_a["start"], span_b["start"])
    end = min(span_a["end"], span_b["end"])

    if start >= end:
        return 0.0

    intersection = end - start
    union = (span_a["end"] - span_a["start"]) + (span_b["end"] - span_b["start"]) - intersection
    return intersection / union

def match_objects(run_a_objects, run_b_objects, overlap_threshold=0.3):
    """
    Returns list of matched (obj_a, obj_b) tuples
    """
    matches = []
    used_b = set()

    for i, obj_a in enumerate(run_a_objects):
        best_match = None
        best_score = 0

        for j, obj_b in enumerate(run_b_objects):
            if j in used_b:
                continue

            score = span_overlap(
                obj_a["evidence"],
                obj_b["evidence"]
            )

            if score > best_score:
                best_score = score
                best_match = j

        if best_score >= overlap_threshold:
            used_b.add(best_match)
            matches.append((obj_a, run_b_objects[best_match]))

    return matches

def align_three_runs(run1, run2, run3):
    """
    Align objects across three runs.
    Returns list of aligned triples: (obj1, obj2, obj3 or None)
    """
    aligned = []

    r1_r2 = match_objects(run1, run2)
    r2_r3 = match_objects(run2, run3)

    for obj1, obj2 in r1_r2:
        obj3 = None
        for r2_obj, r3_obj in r2_r3:
            if r2_obj == obj2:
                obj3 = r3_obj
                break
        aligned.append((obj1, obj2, obj3))

    return aligned

def agreement_rate(run1, run2):
    matches = match_objects(run1, run2)
    union = len(run1) + len(run2) - len(matches)
    return len(matches) / union if union > 0 else 1.0

def polarity_flip_rate(matches):
    flips = 0
    for obj_a, obj_b in matches:
        if obj_a.get("polarity") != obj_b.get("polarity"):
            flips += 1
    return flips / len(matches) if matches else 0.0

def bucket_drift_rate(matches, field="intensity"):
    drift = 0
    for obj_a, obj_b in matches:
        if obj_a.get(field) != obj_b.get(field):
            drift += 1
    return drift / len(matches) if matches else 0.0

def stability_report(journal_runs):
    r1 = journal_runs["1"]["objects"]
    r2 = journal_runs["2"]["objects"]
    r3 = journal_runs["3"]["objects"]

    matches_12 = match_objects(r1, r2)
    matches_23 = match_objects(r2, r3)

    report = {
        "agreement_r1_r2": agreement_rate(r1, r2),
        "agreement_r2_r3": agreement_rate(r2, r3),
        "polarity_flip_rate": polarity_flip_rate(matches_12 + matches_23),
        "intensity_drift_rate": bucket_drift_rate(matches_12 + matches_23)
    }

    return report

journals = load_journals()
llm_runs = load_llm_runs()

print("Number of journals:", len(journals))

for j_id, runs in llm_runs.items():
    print(f"\nJournal {j_id}")
    for run_id, data in runs.items():
        print(
            f"  Run {run_id}:",
            len(data.get("objects", [])),
            "objects"
        )

for journal_id, runs in llm_runs.items():
    print(f"\nJournal {journal_id}")
    print(stability_report(runs))

assert "llm_runs" in globals(), "Run data loading cell first"

len(llm_runs)

!ls -l /content/drive/MyDrive/ashwam_exercise_b/data/llm_runs

from zipfile import ZipFile
import os

zip_path = "/content/drive/MyDrive/ashwam_exercise_b/data/llm_runs/ashwam_exerciseB_data.zip"
extract_dir = "/content/drive/MyDrive/ashwam_exercise_b/data/llm_runs"

with ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extract_dir)

print("Extraction complete. Files now present:")
!ls -l /content/drive/MyDrive/ashwam_exercise_b/data/llm_runs

!ls -l /content/drive/MyDrive/ashwam_exercise_b/data/llm_runs/ashwam_exerciseB_data

import shutil
import os

BASE = "/content/drive/MyDrive/ashwam_exercise_b/data/llm_runs/ashwam_exerciseB_data"
TARGET = "/content/drive/MyDrive/ashwam_exercise_b/data/llm_runs"

for root, _, files in os.walk(BASE):
    for f in files:
        if f.lower().endswith(".json"):
            shutil.move(os.path.join(root, f), os.path.join(TARGET, f))

print("After flattening:")
!ls -l /content/drive/MyDrive/ashwam_exercise_b/data/llm_runs

from collections import defaultdict
import json
import os
import re

def load_llm_runs():
    runs_dir = "/content/drive/MyDrive/ashwam_exercise_b/data/llm_runs"
    runs = defaultdict(dict)

    for fname in os.listdir(runs_dir):
        if not fname.lower().endswith(".json"):
            continue

        # Matches: B001.run1.json
        match = re.match(r"(B\d+)\.run(\d+)\.json", fname)
        if not match:
            print("Skipping unrecognized file:", fname)
            continue

        journal_id = match.group(1)   # B001
        run_id = match.group(2)       # 1 / 2 / 3

        with open(os.path.join(runs_dir, fname), "r") as f:
            runs[journal_id][run_id] = json.load(f)

    return runs

llm_runs = load_llm_runs()

print("Number of journals:", len(llm_runs))
for j, runs in llm_runs.items():
    print(j, sorted(runs.keys()))

sample_journal = llm_runs["B001"]
sample_run = sample_journal["1"]

print(sample_run.keys())

import json
print(json.dumps(sample_run, indent=2)[:1000])

def get_items(run):
    """
    Returns list of extracted semantic items from a run.
    """
    return run.get("items", [])

def text_overlap(a, b):
    """
    Computes token overlap between two evidence spans.
    """
    tokens_a = set(a.lower().split())
    tokens_b = set(b.lower().split())

    if not tokens_a or not tokens_b:
        return 0.0

    intersection = tokens_a & tokens_b
    union = tokens_a | tokens_b

    return len(intersection) / len(union)

def match_items(run_a, run_b, overlap_threshold=0.3):
    items_a = get_items(run_a)
    items_b = get_items(run_b)

    matches = []
    used_b = set()

    for i, a in enumerate(items_a):
        best_j = None
        best_score = 0.0

        for j, b in enumerate(items_b):
            if j in used_b:
                continue

            score = text_overlap(
                a.get("evidence_span", ""),
                b.get("evidence_span", "")
            )

            if score > best_score:
                best_score = score
                best_j = j

        if best_j is not None and best_score >= overlap_threshold:
            used_b.add(best_j)
            matches.append((a, items_b[best_j]))

    return matches

def agreement_rate(run_a, run_b):
    matches = match_items(run_a, run_b)
    union = len(get_items(run_a)) + len(get_items(run_b)) - len(matches)
    return len(matches) / union if union > 0 else 1.0

def polarity_flip_rate(matches):
    flips = 0
    for a, b in matches:
        if a.get("polarity") != b.get("polarity"):
            flips += 1
    return flips / len(matches) if matches else 0.0

def bucket_drift_rate(matches, field):
    drift = 0
    valid = 0

    for a, b in matches:
        if field in a or field in b:
            valid += 1
            if a.get(field) != b.get(field):
                drift += 1

    return drift / valid if valid > 0 else 0.0

def stability_report(journal_runs):
    r1 = journal_runs["1"]
    r2 = journal_runs["2"]
    r3 = journal_runs["3"]

    m12 = match_items(r1, r2)
    m23 = match_items(r2, r3)
    all_matches = m12 + m23

    return {
        "agreement_r1_r2": agreement_rate(r1, r2),
        "agreement_r2_r3": agreement_rate(r2, r3),
        "polarity_flip_rate": polarity_flip_rate(all_matches),
        "intensity_drift_rate": bucket_drift_rate(all_matches, "intensity_bucket"),
        "arousal_drift_rate": bucket_drift_rate(all_matches, "arousal_bucket"),
        "time_drift_rate": bucket_drift_rate(all_matches, "time_bucket")
    }

for journal_id, runs in llm_runs.items():
    print(f"\nJournal {journal_id}")
    report = stability_report(runs)
    for k, v in report.items():
        print(f"  {k}: {round(v, 3)}")

import json

all_reports = {}

for journal_id, runs in llm_runs.items():
    all_reports[journal_id] = stability_report(runs)

print("Rebuilt all_reports:")
print(json.dumps(all_reports, indent=2))

output_path = "/content/drive/MyDrive/ashwam_exercise_b/stability_results.json"

with open(output_path, "w") as f:
    json.dump(all_reports, f, indent=2)

print("Saved results to:", output_path)

!git clone https://github.com/as0396/ashwam-ml-ai-intern-aryaman-singh.git

# Commented out IPython magic to ensure Python compatibility.
# %cd ashwam-ml-ai-intern-aryaman-singh

!cp -r /content/drive/MyDrive/ashwam_exercise_b/* .

!ls

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat << 'EOF' > main.py
# from src.matcher import match_items
# from src.metrics import agreement_rate, polarity_flip_rate, bucket_drift_rate
# from src.loader import load_llm_runs
# 
# def stability_report(journal_runs):
#     r1 = journal_runs["1"]
#     r2 = journal_runs["2"]
#     r3 = journal_runs["3"]
# 
#     m12 = match_items(r1, r2)
#     m23 = match_items(r2, r3)
#     all_matches = m12 + m23
# 
#     return {
#         "agreement_r1_r2": agreement_rate(r1, r2),
#         "agreement_r2_r3": agreement_rate(r2, r3),
#         "polarity_flip_rate": polarity_flip_rate(all_matches),
#         "intensity_drift_rate": bucket_drift_rate(all_matches, "intensity_bucket"),
#         "arousal_drift_rate": bucket_drift_rate(all_matches, "arousal_bucket"),
#         "time_drift_rate": bucket_drift_rate(all_matches, "time_bucket"),
#     }
# 
# if __name__ == "__main__":
#     llm_runs = load_llm_runs()
#     for journal_id, runs in llm_runs.items():
#         print(journal_id, stability_report(runs))
# EOF
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat << 'EOF' > README.md
# ## Exercise B — Run-to-Run Variance & Stability Analysis
# 
# ### What did I build?
# I built a deterministic framework to measure run-to-run instability in LLM outputs without canonical labels. The system aligns extracted items across multiple runs using evidence-text overlap and computes safety-critical stability metrics.
# 
# ### Key assumptions
# - Evidence grounding is the strongest signal for semantic identity
# - Polarity changes are high-risk and unacceptable
# - Bucket drift is acceptable only within bounded ranges
# - Stability is evaluated per journal independently
# 
# ### System breakdown
# - Loader: reads LLM run outputs
# - Matcher: aligns items via evidence overlap
# - Metrics: agreement rate, polarity flip rate, bucket drift rate
# - Report: per-journal stability summary
# 
# ### Determinism & safety controls
# - Evidence-first deterministic matching
# - Explicit polarity flip detection
# - No hallucinated merges
# - Abstains when evidence is insufficient
# 
# ### Evaluation strategy
# Measured stability across runs using:
# - Agreement rate
# - Polarity flip rate
# - Bucket drift rate (intensity, arousal, time)
# 
# ### Edge cases & limitations
# - Token overlap may underperform on very short spans
# - Does not resolve implicit or cross-sentence evidence
# - Bucket boundaries are heuristic
# 
# ### Future improvements
# - Learned similarity thresholds
# - Confidence-weighted aggregation
# - Production monitoring dashboards
# EOF
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# echo "python>=3.9" > requirements.txt
#

!ls

!git add .

!git commit -m "Final Exercise B submission"

!git config --global user.name "Aryaman Singh"

!git config --global user.email "as0396@users.noreply.github.com"

!git commit -m "Final Exercise B submission"

!git push

!git config --global user.name "as0396"

!git push https://as0396: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@github.com/as0396/ashwam-ml-ai-intern-aryaman-singh.git

!zip -r submission.zip README.md main.py requirements.txt stability_results.json src